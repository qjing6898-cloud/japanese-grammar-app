import streamlit as st
import pandas as pd
import google.generativeai as genai
from datetime import datetime
import json
import gspread
import pytz 
import time
from gtts import gTTS # å¼•å…¥è¯­éŸ³åº“
import io

# --- 1. é…ç½®ä½ çš„ AI ---
try:
    # ä» Streamlit Cloud Secrets å®‰å…¨è¯»å– Key
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash')
except KeyError:
    st.error("æ— æ³•è¯»å– Gemini API Keyã€‚è¯·åœ¨ Streamlit Cloud Secrets ä¸­æ£€æŸ¥ GOOGLE_API_KEY é…ç½®ã€‚")
except Exception as e:
    st.error(f"AI é…ç½®é”™è¯¯: {e}")

# --- 2. æ•°æ®åº“è¿æ¥é…ç½® (Google Sheets) ---
SHEET_TITLE = "Japanese_Grammar_History"
# âš ï¸âš ï¸âš ï¸ è¯·ä¿æŒä½ å·²ç»é…ç½®å¥½çš„ Google Sheets å®Œæ•´ç½‘å€ä¸å˜ï¼
SHEET_URL = "https://docs.google.com/spreadsheets/d/1xrXmiV5yEYIC4lDfgjk79vQDNVHYZugW6XUReZbHWjY/edit?gid=0#gid=0" 

@st.cache_resource(ttl=3600) # ç¼“å­˜è¿æ¥
def get_sheets_client():
    try:
        if "GCP_JSON_STRING" in st.secrets:
            key_dict = json.loads(st.secrets["GCP_JSON_STRING"])
            gc = gspread.service_account_from_dict(key_dict)
            return gc
        elif "gcp_service_account" in st.secrets:
            gcp_sa = st.secrets["gcp_service_account"]
            gc = gspread.service_account_from_dict(gcp_sa)
            return gc
        else:
            st.warning("æœªæ‰¾åˆ° Google Cloud å‡­è¯ (GCP_JSON_STRING)ã€‚")
            return None
    except Exception as e:
        st.error(f"Google Sheets è®¤è¯å¤±è´¥: {e}")
        return None

# ç¼“å­˜æ•°æ®è¯»å–
@st.cache_data(ttl=60) 
def load_history():
    """ä» Google Sheets è¯»å–å†å²è®°å½•"""
    gc = get_sheets_client()
    if not gc: return pd.DataFrame()
    
    try:
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.sheet1
        df = pd.DataFrame(worksheet.get_all_records())
        
        if 'data_json' in df.columns:
            # è§£æ JSON å­—ç¬¦ä¸²
            df['data'] = df['data_json'].apply(lambda x: json.loads(x) if x else {})
            # æå–è¯­è¨€å­—æ®µ
            df['language'] = df['data'].apply(lambda x: x.get('language', 'æ—¥è¯­') if isinstance(x, dict) else 'æœªçŸ¥')
            
        return df.iloc[::-1] # å€’åº
    except gspread.exceptions.SpreadsheetNotFound:
        st.warning(f"Google è¡¨æ ¼ '{SHEET_TITLE}' ä¸å­˜åœ¨æˆ–æ— è®¿é—®æƒé™ã€‚")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"åŠ è½½å†å²è®°å½•å¤±è´¥: {e}") 
        return pd.DataFrame()


def save_record(sentence, result_data):
    """å°†æ–°çš„è®°å½•å†™å…¥ Google Sheets"""
    gc = get_sheets_client()
    if not gc: return
    
    try:
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.sheet1
        
        tz = pytz.timezone('Asia/Shanghai')
        timestamp_str = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

        new_row = [
            timestamp_str,
            sentence,
            json.dumps(result_data, ensure_ascii=False), 
            st.session_state.get('user_id', 'Unknown')
        ]
        
        if not worksheet.row_values(1):
            worksheet.append_row(['timestamp', 'sentence', 'data_json', 'user'])

        worksheet.append_row(new_row)
    except Exception as e:
        st.error(f"ä¿å­˜è®°å½•åˆ° Google Sheets å¤±è´¥: {e}")

# æ‰¹é‡åˆ é™¤å‡½æ•°
def delete_records_by_bulk(timestamps_list):
    """æ ¹æ®æ—¶é—´æˆ³åˆ—è¡¨æ‰¹é‡åˆ é™¤ Google Sheets ä¸­çš„è®°å½•"""
    gc = get_sheets_client()
    if not gc or not timestamps_list: return False
    
    try:
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.sheet1
        
        timestamps_col = worksheet.col_values(1)
        
        rows_to_delete = []
        for ts in timestamps_list:
            try:
                row_index = timestamps_col.index(ts) + 1
                rows_to_delete.append(row_index)
            except ValueError:
                continue
        
        if not rows_to_delete:
            st.toast("âš ï¸ æœªæ‰¾åˆ°è¦åˆ é™¤çš„è®°å½•ã€‚", icon="âš ï¸")
            return False

        # æ ¸å¿ƒï¼šæŒ‰è¡Œå·ä»å¤§åˆ°å°æ’åº
        rows_to_delete.sort(reverse=True)
        
        success_count = 0
        for row_idx in rows_to_delete:
            worksheet.delete_rows(row_idx)
            success_count += 1
        
        st.toast(f"âœ… æˆåŠŸåˆ é™¤ {success_count} æ¡è®°å½•ï¼", icon="ğŸ—‘ï¸")
        return True
            
    except Exception as e:
        st.error(f"æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
        return False

# åˆ—åæ˜ å°„
COLUMN_MAPPING = {
    "word": "å•è¯/åŸæ–‡",
    "reading": "å‘éŸ³",
    "pos_meaning": "è¯æ€§ / å«ä¹‰", 
    "grammar": "è¯­æ³•è¯´æ˜",
    "standard": "åŸå‹"
}


# --- è¾…åŠ©å‡½æ•°ï¼šçŠ¶æ€åŒæ­¥ ---

def update_individual_selection(ts):
    """å½“å•ä¸ªå¤é€‰æ¡†è¢«ç‚¹å‡»æ—¶è°ƒç”¨"""
    checkbox_key = f"sel_{ts}"
    is_checked = st.session_state[checkbox_key] 
    st.session_state.delete_selections[ts] = is_checked
    if not is_checked and st.session_state.select_all:
        st.session_state.select_all = False

def update_selections():
    """å½“ç‚¹å‡»å…¨é€‰æ—¶è°ƒç”¨"""
    select_all_state = st.session_state.select_all
    
    history_df = load_history() 
    
    # é‡æ–°åº”ç”¨å½“å‰çš„è¿‡æ»¤é€»è¾‘ï¼Œç¡®ä¿å…¨é€‰åªé’ˆå¯¹å½“å‰å¯è§çš„è®°å½•
    filter_lang = st.session_state.get('filter_language', None)
    search_query = st.session_state.get('search_query', '')
    
    filtered_df = history_df.copy()
    if filter_lang:
        filtered_df = filtered_df[filtered_df['language'] == filter_lang]
    if search_query:
        filtered_df = filtered_df[
            filtered_df['sentence'].str.contains(search_query, case=False, na=False) | 
            (filtered_df['data'].astype(str).str.contains(search_query, case=False, na=False))
        ]
        
    for ts in filtered_df['timestamp']:
        st.session_state.delete_selections[ts] = select_all_state
        if f"sel_{ts}" in st.session_state:
            st.session_state[f"sel_{ts}"] = select_all_state

def bulk_delete_callback(timestamps_to_delete):
    """åˆ é™¤æŒ‰é’®çš„å›è°ƒå‡½æ•°"""
    if not timestamps_to_delete:
        st.toast("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€æ¡è®°å½•è¿›è¡Œåˆ é™¤ã€‚", icon="âš ï¸")
        return

    if delete_records_by_bulk(timestamps_to_delete):
        st.session_state.select_all = False
        st.session_state.delete_selections = {}
        time.sleep(1) 
        load_history.clear()
        # å›è°ƒç»“æŸåä¼šè‡ªåŠ¨åˆ·æ–°

def text_to_speech(text, lang_name):
    """ä½¿ç”¨ gTTS ç”Ÿæˆè¯­éŸ³ï¼Œè¿”å›éŸ³é¢‘å­—èŠ‚æµ"""
    # ç®€å•çš„è¯­è¨€ä»£ç æ˜ å°„
    lang_map = {
        'è‹±è¯­': 'en', 'æ—¥è¯­': 'ja', 'ä¸­æ–‡': 'zh-cn', 'æ³•è¯­': 'fr', 
        'éŸ©è¯­': 'ko', 'è¥¿ç­ç‰™è¯­': 'es', 'å¾·è¯­': 'de', 'ä¿„è¯­': 'ru', 'æ„å¤§åˆ©è¯­': 'it'
    }
    # é»˜è®¤ä½¿ç”¨è‹±è¯­ï¼Œå¦‚æœåŒ¹é…ä¸åˆ°
    lang_code = lang_map.get(lang_name, 'en') 
    
    try:
        tts = gTTS(text=text, lang=lang_code)
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        return fp
    except Exception as e:
        print(f"TTS Error: {e}")
        return None

# --- 4. æ ¸å¿ƒåŠŸèƒ½ï¼šAI åˆ†æ ---
def analyze_with_ai(text):
    prompt = f"""
    è¯·ä½œä¸ºä¸€ä½ç²¾é€šå…¨çƒè¯­è¨€çš„è¯­è¨€å­¦ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š
    â€œ{text}â€
    
    è¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
    1. **è‡ªåŠ¨è¯†åˆ«** è¾“å…¥æ–‡æœ¬çš„è¯­è¨€ï¼ˆä¾‹å¦‚ï¼šæ—¥è¯­ã€è‹±è¯­ã€æ³•è¯­ã€éŸ©è¯­ã€ä¸­æ–‡ã€è¥¿ç­ç‰™è¯­ç­‰ï¼‰ã€‚
    2. å°†æ–‡æœ¬ç¿»è¯‘æˆæµç•…çš„ **ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰**ã€‚
    3. åˆ†ææ–‡æœ¬ä¸­çš„è¯­æ°”ã€æƒ¯ç”¨è¯­ã€è¯­æ³•ç»“æ„æˆ–æ–­å¥é€»è¾‘ã€‚
    4. å¯¹æ–‡æœ¬è¿›è¡Œé€è¯/é€ç»“æ„æ‹†è§£åˆ†æã€‚

    è¯·è¾“å‡ºä¸€ä¸ªä¸¥æ ¼çš„ JSON æ ¼å¼å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å››ä¸ªå­—æ®µï¼š
    1. "language": è¯†åˆ«å‡ºçš„è¯­è¨€åç§° (å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "è‹±è¯­", "æ—¥è¯­")ã€‚
    2. "translation": ä¸­æ–‡ç¿»è¯‘ã€‚
    3. "nuances": è¯¦ç»†çš„è¯­æ³•ç¬”è®°ã€æƒ¯ç”¨è¯­è§£é‡Šæˆ–æ–‡åŒ–èƒŒæ™¯è¯´æ˜ã€‚
    4. "structure": ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«é€è¯æ‹†è§£ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
       - "word": åŸæ–‡å•è¯/è¯ç»„
       - "reading": å‘éŸ³æ³¨éŸ³ (è‹±è¯­è¯·æä¾›IPAéŸ³æ ‡ï¼Œæ—¥è¯­æä¾›ç½—é©¬éŸ³ï¼Œä¸­æ–‡æä¾›æ‹¼éŸ³)
       - "pos_meaning": è¯æ€§åŠä¸­æ–‡å«ä¹‰
       - "grammar": ç®€çŸ­è¯­æ³•è¯´æ˜ (æ—¶æ€ã€å˜ä½ç­‰)
       - "standard": åŸå‹/æ ‡å‡†å½¢å¼ (å¦‚åŠ¨è¯åŸå½¢)

    è¯·ç¡®ä¿è¾“å‡ºæ˜¯åˆæ³•çš„ JSON æ ¼å¼ã€‚
    """
    
    try:
        response = model.generate_content(prompt)
        clean_text = response.text.replace('```json', '').replace('```', '').strip()
        result = json.loads(clean_text)
        
        if "structure" not in result or "translation" not in result:
             return {"error": "AIè¿”å›æ ¼å¼ä¸å®Œæ•´", "structure": []}
            
        return result
        
    except Exception as e:
        return {"error": f"AIåˆ†æå¤±è´¥: {e}", "structure": []}

# --- 3. é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="å…¨èƒ½è¯­è¨€ä¼´ä¾£",
    page_icon="ğŸŒ",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# æ—¶å°šçš„ UI æ ·å¼
st.markdown("""
<style>
    /* éšè—é»˜è®¤èœå• */
    #MainMenu {visibility: hidden;}
    header {visibility: hidden;}
    footer {visibility: hidden;}
    
    /* è¡¨æ ¼è‡ªåŠ¨æ¢è¡Œ */
    td { white-space: normal !important; word-wrap: break-word !important; }
    
    /* å¡ç‰‡å¼è®¾è®¡ */
    .stApp { background-color: #fafafa; }
    .css-1r6slb0 { background-color: #ffffff; padding: 2rem; border-radius: 15px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); }
    
    /* è¯­è¨€æ ‡ç­¾ */
    .lang-tag {
        background-color: #e3f2fd;
        color: #1976d2;
        padding: 4px 10px;
        border-radius: 16px;
        font-size: 13px;
        font-weight: 600;
        margin-right: 10px;
        display: inline-block;
    }
    
    /* ç¿»è¯‘æ¡†ä¼˜åŒ– */
    .trans-text { font-size: 18px; color: #333; line-height: 1.6; }
    
    /* è¯­æ³•ç¬”è®° */
    .nuance-box {
        background-color: #fff8e1;
        border-left: 4px solid #ffc107;
        padding: 15px;
        border-radius: 4px;
        color: #5d4037;
    }
</style>
""", unsafe_allow_html=True)


if 'user_id' not in st.session_state:
    st.session_state['user_id'] = 'ç”¨æˆ·A'

# --- 5. ä¾§è¾¹æ ï¼šä¸ªäººä¸­å¿ƒä¸ç»Ÿè®¡ ---
with st.sidebar:
    st.header("ğŸ‘¤ ä¸ªäººä¸­å¿ƒ")
    st.session_state['user_id'] = st.text_input("æ˜µç§°", value=st.session_state['user_id'])
    
    st.markdown("---")
    st.subheader("ğŸ“Š å­¦ä¹ ä»ªè¡¨ç›˜")
    
    # å®æ—¶ç»Ÿè®¡
    hist_df_stats = load_history()
    if not hist_df_stats.empty:
        total_queries = len(hist_df_stats)
        langs_learned = hist_df_stats['language'].nunique()
        top_lang = hist_df_stats['language'].mode()[0] if not hist_df_stats.empty else "æ— "
        
        c1, c2 = st.columns(2)
        c1.metric("æ€»æŸ¥è¯¢", total_queries)
        c2.metric("æ¶‰çŒè¯­è¨€", langs_learned)
        st.metric("æœ€çˆ±è¯­è¨€", top_lang)
    else:
        st.info("æš‚æ— å­¦ä¹ æ•°æ®")
        
    st.markdown("---")
    st.markdown("ğŸ’¡ *Made with Streamlit & Gemini*")

# --- 6. ä¸»ç•Œé¢ UI ---
st.title("ğŸŒ å…¨èƒ½è¯­è¨€ä¼´ä¾£")
st.caption("AI é©±åŠ¨çš„å¤šè¯­ç§ç¿»è¯‘ã€è¯­æ³•è§£æä¸å‘éŸ³åŠ©æ‰‹")

# è¾“å…¥åŒº
with st.container():
    sentence = st.text_area("", height=100, placeholder="åœ¨æ­¤è¾“å…¥æ—¥è¯­ã€è‹±è¯­ã€éŸ©è¯­æˆ–ä»»ä½•ä½ æƒ³å­¦ä¹ çš„è¯­è¨€å¥å­...")
    
    col_btn, col_empty = st.columns([1, 3])
    with col_btn:
        analyze_btn = st.button("âœ¨ æ·±åº¦è§£æ", type="primary", use_container_width=True)

    if analyze_btn:
        if not sentence:
            st.toast("âš ï¸ è¯·å…ˆè¾“å…¥å¥å­ï¼", icon="âœï¸")
        else:
            with st.spinner('ğŸ¤– AI æ­£åœ¨è¯†åˆ«è¯­è¨€ã€ç”Ÿæˆå‘éŸ³å¹¶æ‹†è§£è¯­æ³•...'):
                ai_result = analyze_with_ai(sentence)
                
                if "error" in ai_result:
                    st.error(ai_result["error"])
                else:
                    save_record(sentence, ai_result)
                    load_history.clear() # æ¸…é™¤ç¼“å­˜
                    
                    st.toast("âœ… è§£æå®Œæˆï¼å·²ä¿å­˜åˆ°äº‘ç«¯ã€‚", icon="ğŸ‰")
                    
                    # --- ç»“æœå±•ç¤ºåŒº (ä½¿ç”¨ Tabs ä¼˜åŒ–å¸ƒå±€) ---
                    st.markdown("###") # Spacer
                    
                    # ç”Ÿæˆè¯­éŸ³
                    lang_name = ai_result.get('language', 'è‹±è¯­')
                    audio_fp = text_to_speech(sentence, lang_name)
                    
                    # é¡¶éƒ¨åŸºæœ¬ä¿¡æ¯å¡ç‰‡
                    with st.container():
                        c_lang, c_audio = st.columns([0.2, 0.8])
                        with c_lang:
                            st.markdown(f"<span class='lang-tag'>{lang_name}</span>", unsafe_allow_html=True)
                        with c_audio:
                            if audio_fp:
                                st.audio(audio_fp, format='audio/mp3')
                    
                    # ä½¿ç”¨ Tabs åˆ†é¡µå±•ç¤ºï¼Œç•Œé¢æ›´æ¸…çˆ½
                    tab1, tab2, tab3 = st.tabs(["ğŸ“ ç¿»è¯‘ä¸ç¬”è®°", "ğŸ§© ç»“æ„æ‹†è§£", "ğŸ” åŸå§‹æ•°æ®"])
                    
                    with tab1:
                        st.markdown("#### ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç¿»è¯‘")
                        st.markdown(f"<div class='trans-text'>{ai_result.get('translation', '')}</div>", unsafe_allow_html=True)
                        
                        st.markdown("#### ğŸ’¡ è¯­æ³•ä¸æ–‡åŒ–ç¬”è®°")
                        st.markdown(f"""
                        <div class="nuance-box">
                            {ai_result.get('nuances', 'æ— ç‰¹æ®Šè¯´æ˜').replace(chr(10), '<br>')}
                        </div>
                        """, unsafe_allow_html=True)
                        
                    with tab2:
                        st.markdown("#### é€è¯æ‹†è§£")
                        df = pd.DataFrame(ai_result.get('structure', []))
                        if not df.empty:
                            df_display = df.rename(columns=COLUMN_MAPPING)
                            st.table(df_display)
                        else:
                            st.info("æ— æ³•ç”Ÿæˆç»“æ„è¡¨æ ¼")
                            
                    with tab3:
                        st.json(ai_result)

st.divider()

# --- 7. å­¦ä¹ è¶³è¿¹ (å‡çº§ç‰ˆ) ---
st.subheader("ğŸ“š å­¦ä¹ è¶³è¿¹")

# åˆå§‹åŒ– session_state
if 'select_all' not in st.session_state:
    st.session_state.select_all = False
if 'delete_selections' not in st.session_state:
    st.session_state.delete_selections = {}
if 'search_query' not in st.session_state:
    st.session_state.search_query = ''
if 'filter_language' not in st.session_state:
    st.session_state.filter_language = None

# åŠ è½½æ•°æ®
history_df = load_history()

if not history_df.empty and 'timestamp' in history_df.columns:
    
    # ğŸŒŸ é¡¶éƒ¨å·¥å…·æ ï¼šç­›é€‰ + å¯¼å‡º
    col_filter, col_export = st.columns([0.8, 0.2])
    
    with col_filter:
        # è¯­è¨€ç­›é€‰æŒ‰é’®
        available_languages = history_df['language'].unique().tolist()
        if len(available_languages) > 0:
            # st.markdown("##### è¯­è¨€ç­›é€‰")
            cols = st.columns(len(available_languages) + 1)
            def set_lang_filter(lang):
                if st.session_state.filter_language == lang:
                    st.session_state.filter_language = None
                else:
                    st.session_state.filter_language = lang
                st.session_state.select_all = False 
                st.session_state.delete_selections = {}

            for i, lang in enumerate(available_languages):
                btn_type = "primary" if st.session_state.filter_language == lang else "secondary"
                if cols[i].button(lang, key=f"filter_btn_{lang}", type=btn_type):
                    set_lang_filter(lang)
                    st.rerun()
    
    with col_export:
        # ğŸŒŸ å¯¼å‡ºåŠŸèƒ½
        csv = history_df.to_csv(index=False).encode('utf-8-sig') # è§£å†³ä¸­æ–‡ä¹±ç 
        st.download_button(
            label="ğŸ“¥ å¯¼å‡º CSV",
            data=csv,
            file_name=f'learning_history_{datetime.now().strftime("%Y%m%d")}.csv',
            mime='text/csv',
        )

    # ğŸŒŸ æ‰§è¡Œè¿‡æ»¤
    filtered_df = history_df.copy()
    if st.session_state.filter_language:
        filtered_df = filtered_df[filtered_df['language'] == st.session_state.filter_language]

    search_query = st.text_input("ğŸ” æœç´¢å†å²:", placeholder="æœç´¢åŸæ–‡ã€ç¿»è¯‘æˆ–ç¬”è®°...", key='search_query')
    if search_query:
        filtered_df = filtered_df[
            filtered_df['sentence'].str.contains(search_query, case=False, na=False) | 
            (filtered_df['data'].astype(str).str.contains(search_query, case=False, na=False))
        ]

    # --- æ‰¹é‡åˆ é™¤é€»è¾‘ ---
    if not filtered_df.empty:
        c_sel, c_del, c_space = st.columns([0.15, 0.35, 0.5])
        c_sel.checkbox("å…¨é€‰", key="select_all", on_change=update_selections)

        timestamps_to_delete = [
            ts for ts, is_checked in st.session_state.delete_selections.items() 
            if is_checked and ts in filtered_df['timestamp'].values
        ]
        
        c_del.button(
            "ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", 
            type="primary", 
            key="bulk_delete_main_btn",
            on_click=bulk_delete_callback,
            args=(timestamps_to_delete,)
        )

    # --- åˆ—è¡¨æ˜¾ç¤º ---
    if filtered_df.empty:
        st.info("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•")
    else:
        for index, item in filtered_df.iterrows():
            timestamp = item['timestamp']
            display_sentence = item['sentence'][:30] + '...' if len(item['sentence']) > 30 else item['sentence']
            lang_label = item.get('language', 'æœªçŸ¥')
            
            # ä½¿ç”¨ container å¢åŠ å¡ç‰‡æ„Ÿ
            with st.container():
                c_check, c_content = st.columns([0.05, 0.95])
                
                with c_check:
                    checkbox_key = f"sel_{timestamp}"
                    if checkbox_key not in st.session_state:
                        st.session_state[checkbox_key] = st.session_state.delete_selections.get(timestamp, False)
                    st.checkbox("", key=checkbox_key, on_change=update_individual_selection, args=(timestamp,), label_visibility="hidden")
                
                with c_content:
                    with st.expander(f"[{lang_label}] {display_sentence}"):
                        st.caption(f"ğŸ•’ {timestamp} | ğŸ‘¤ {item['user']}")
                        
                        data = item.get('data', {})
                        if data and "structure" in data:
                            # è¿™é‡Œä¹Ÿå¯ä»¥åŠ  TTS
                            if st.button("ğŸ”Š æœ—è¯»", key=f"tts_{timestamp}"):
                                audio_bytes = text_to_speech(item['sentence'], lang_label)
                                if audio_bytes:
                                    st.audio(audio_bytes, format='audio/mp3')

                            st.markdown(f"**ç¿»è¯‘ï¼š** {data.get('translation', '')}")
                            
                            # ç®€å•çš„ Tab å¸ƒå±€ç”¨äºå†å²è®°å½•
                            h_tab1, h_tab2 = st.tabs(["ç»“æ„è¡¨", "ç¬”è®°"])
                            with h_tab1:
                                h_df = pd.DataFrame(data['structure'])
                                st.table(h_df.rename(columns=COLUMN_MAPPING))
                            with h_tab2:
                                st.info(data.get('nuances', 'æ— ç¬”è®°'))
                        else:
                            st.warning("æ•°æ®æ— æ³•è§£æ")

else:
    st.info("ğŸŒŸ æ¬¢è¿ä½¿ç”¨ï¼è¾“å…¥ç¬¬ä¸€ä¸ªå¥å­å¼€å§‹ä½ çš„è¯­è¨€ä¹‹æ—…å§ï¼")
