import streamlit as st
import pandas as pd
import google.generativeai as genai
from datetime import datetime
import json
import gspread
import pytz 
import time
from gtts import gTTS 
import io

# --- 1. é…ç½®ä½ çš„ AI ---
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash')
except KeyError:
    st.error("æ— æ³•è¯»å– Gemini API Keyã€‚è¯·åœ¨ Streamlit Cloud Secrets ä¸­æ£€æŸ¥ GOOGLE_API_KEY é…ç½®ã€‚")
except Exception as e:
    st.error(f"AI é…ç½®é”™è¯¯: {e}")

# --- 2. æ•°æ®åº“è¿æ¥é…ç½® (Google Sheets) ---
SHEET_TITLE = "Japanese_Grammar_History"
SHEET_URL = "https://docs.google.com/spreadsheets/d/1xrXmiV5yEYIC4lDfgjk79vQDNVHYZugW6XUReZbHWjY/edit?gid=0#gid=0" 

@st.cache_resource(ttl=3600)
def get_sheets_client():
    try:
        if "GCP_JSON_STRING" in st.secrets:
            key_dict = json.loads(st.secrets["GCP_JSON_STRING"])
            gc = gspread.service_account_from_dict(key_dict)
            return gc
        elif "gcp_service_account" in st.secrets:
            gcp_sa = st.secrets["gcp_service_account"]
            gc = gspread.service_account_from_dict(gcp_sa)
            return gc
        else:
            st.warning("æœªæ‰¾åˆ° Google Cloud å‡­è¯ (GCP_JSON_STRING)ã€‚")
            return None
    except Exception as e:
        st.error(f"Google Sheets è®¤è¯å¤±è´¥: {e}")
        return None

# ç¼“å­˜æ•°æ®è¯»å–
@st.cache_data(ttl=60) 
def load_history():
    """ä» Google Sheets è¯»å–å†å²è®°å½•"""
    gc = get_sheets_client()
    if not gc: return pd.DataFrame()
    
    try:
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.sheet1
        df = pd.DataFrame(worksheet.get_all_records())
        
        if 'data_json' in df.columns:
            df['data'] = df['data_json'].apply(lambda x: json.loads(x) if x else {})
            df['language'] = df['data'].apply(lambda x: x.get('language', 'æ—¥è¯­') if isinstance(x, dict) else 'æœªçŸ¥')
            
        return df.iloc[::-1] # å€’åº
    except gspread.exceptions.SpreadsheetNotFound:
        st.warning(f"Google è¡¨æ ¼ '{SHEET_TITLE}' ä¸å­˜åœ¨æˆ–æ— è®¿é—®æƒé™ã€‚")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"åŠ è½½å†å²è®°å½•å¤±è´¥: {e}") 
        return pd.DataFrame()


def save_record(sentence, result_data):
    """å°†æ–°çš„è®°å½•å†™å…¥ Google Sheets"""
    gc = get_sheets_client()
    if not gc: return
    
    try:
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.sheet1
        
        tz = pytz.timezone('Asia/Shanghai')
        timestamp_str = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

        new_row = [
            timestamp_str,
            sentence,
            json.dumps(result_data, ensure_ascii=False), 
            st.session_state.get('user_id', 'Unknown')
        ]
        
        if not worksheet.row_values(1):
            worksheet.append_row(['timestamp', 'sentence', 'data_json', 'user'])

        worksheet.append_row(new_row)
    except Exception as e:
        st.error(f"ä¿å­˜è®°å½•åˆ° Google Sheets å¤±è´¥: {e}")


def delete_records_by_bulk(timestamps_list):
    """æ ¹æ®æ—¶é—´æˆ³åˆ—è¡¨æ‰¹é‡åˆ é™¤ Google Sheets ä¸­çš„è®°å½•"""
    gc = get_sheets_client()
    if not gc or not timestamps_list: return False
    
    try:
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.sheet1
        
        timestamps_col = worksheet.col_values(1)
        
        rows_to_delete = []
        for ts in timestamps_list:
            try:
                row_index = timestamps_col.index(ts) + 1
                rows_to_delete.append(row_index)
            except ValueError:
                continue
        
        if not rows_to_delete:
            st.toast("âš ï¸ æœªæ‰¾åˆ°è¦åˆ é™¤çš„è®°å½•ã€‚", icon="âš ï¸")
            return False

        rows_to_delete.sort(reverse=True)
        
        success_count = 0
        for row_idx in rows_to_delete:
            worksheet.delete_rows(row_idx)
            success_count += 1
        
        st.toast(f"âœ… æˆåŠŸåˆ é™¤ {success_count} æ¡è®°å½•ï¼", icon="ğŸ—‘ï¸")
        return True
            
    except Exception as e:
        st.error(f"æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
        return False

# åˆ—åæ˜ å°„
COLUMN_MAPPING = {
    "word": "å•è¯/åŸæ–‡",
    "reading": "å‘éŸ³",
    "pos_meaning": "è¯æ€§ / å«ä¹‰", 
    "grammar": "è¯­æ³•è¯´æ˜",
    "standard": "åŸå‹"
}


# --- è¾…åŠ©å‡½æ•°ï¼šçŠ¶æ€åŒæ­¥ ---

def update_individual_selection(ts):
    checkbox_key = f"sel_{ts}"
    is_checked = st.session_state[checkbox_key] 
    st.session_state.delete_selections[ts] = is_checked
    if not is_checked and st.session_state.select_all:
        st.session_state.select_all = False

def update_selections():
    select_all_state = st.session_state.select_all
    
    history_df = load_history() 
    
    filter_lang = st.session_state.get('filter_language', None)
    search_query = st.session_state.get('search_query', '')
    
    filtered_df = history_df.copy()
    if filter_lang:
        filtered_df = filtered_df[filtered_df['language'] == filter_lang]
    if search_query:
        filtered_df = filtered_df[
            filtered_df['sentence'].str.contains(search_query, case=False, na=False) | 
            (filtered_df['data'].astype(str).str.contains(search_query, case=False, na=False))
        ]
        
    for ts in filtered_df['timestamp']:
        st.session_state.delete_selections[ts] = select_all_state
        if f"sel_{ts}" in st.session_state:
            st.session_state[f"sel_{ts}"] = select_all_state

def bulk_delete_callback(timestamps_to_delete):
    if not timestamps_to_delete:
        st.toast("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€æ¡è®°å½•è¿›è¡Œåˆ é™¤ã€‚", icon="âš ï¸")
        return

    if delete_records_by_bulk(timestamps_to_delete):
        st.session_state.select_all = False
        st.session_state.delete_selections = {}
        time.sleep(1) 
        load_history.clear()

def text_to_speech(text, lang_name):
    """ä½¿ç”¨ gTTS ç”Ÿæˆè¯­éŸ³ï¼Œè¿”å›éŸ³é¢‘å­—èŠ‚æµ"""
    lang_map = {
        'è‹±è¯­': 'en', 'æ—¥è¯­': 'ja', 'ä¸­æ–‡': 'zh-cn', 'æ³•è¯­': 'fr', 
        'éŸ©è¯­': 'ko', 'è¥¿ç­ç‰™è¯­': 'es', 'å¾·è¯­': 'de', 'ä¿„è¯­': 'ru', 'æ„å¤§åˆ©è¯­': 'it'
    }
    lang_code = lang_map.get(lang_name, 'en') 
    
    try:
        tts = gTTS(text=text, lang=lang_code)
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        return fp
    except Exception as e:
        print(f"TTS Error: {e}")
        return None

# --- 4. æ ¸å¿ƒåŠŸèƒ½ï¼šAI åˆ†æ (æ–°å¢ Correction) ---
def analyze_with_ai(text):
    # ğŸŒŸ å®ç”¨åŠŸèƒ½ä¸€ï¼šæ–°å¢è¯­æ³•çº é”™å’Œæ¶¦è‰²è¦æ±‚
    prompt = f"""
    è¯·ä½œä¸ºä¸€ä½ç²¾é€šå…¨çƒè¯­è¨€çš„è¯­è¨€å­¦ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š
    â€œ{text}â€
    
    è¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
    1. **è‡ªåŠ¨è¯†åˆ«** è¾“å…¥æ–‡æœ¬çš„è¯­è¨€ï¼ˆä¾‹å¦‚ï¼šæ—¥è¯­ã€è‹±è¯­ã€æ³•è¯­ã€éŸ©è¯­ã€ä¸­æ–‡ã€è¥¿ç­ç‰™è¯­ç­‰ï¼‰ã€‚
    2. **æ£€æŸ¥å’Œæ¶¦è‰²ï¼š** æ£€æŸ¥åŸæ–‡æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯ã€è¡¨è¾¾ä¸è‡ªç„¶æˆ–ä¸åœ°é“çš„åœ°æ–¹ã€‚
        - å¦‚æœæœ‰é”™è¯¯æˆ–ä¸åœ°é“ï¼Œè¯·æä¾›ä¸€ä¸ª**å®Œå…¨ä¿®æ­£ä¸”åœ°é“çš„ç‰ˆæœ¬**ã€‚
        - å¦‚æœåŸæ–‡å®Œç¾æˆ–éå¸¸åœ°é“ï¼Œè¯·è¿”å›åŸæ–‡ã€‚
    3. å°†æ–‡æœ¬ç¿»è¯‘æˆæµç•…çš„ **ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰**ã€‚
    4. åˆ†ææ–‡æœ¬ä¸­çš„è¯­æ°”ã€æƒ¯ç”¨è¯­ã€è¯­æ³•ç»“æ„æˆ–æ–­å¥é€»è¾‘ã€‚
    5. å¯¹æ–‡æœ¬è¿›è¡Œé€è¯/é€ç»“æ„æ‹†è§£åˆ†æã€‚

    è¯·è¾“å‡ºä¸€ä¸ªä¸¥æ ¼çš„ JSON æ ¼å¼å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹äº”ä¸ªå­—æ®µï¼š
    1. "language": è¯†åˆ«å‡ºçš„è¯­è¨€åç§° (å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "è‹±è¯­", "æ—¥è¯­")ã€‚
    2. "correction": **ä¿®æ­£/æ¶¦è‰²åçš„ç‰ˆæœ¬** (å¦‚æœåŸæ–‡æ— é”™ï¼Œåˆ™è¿”å›åŸæ–‡)ã€‚
    3. "translation": ä¸­æ–‡ç¿»è¯‘ã€‚
    4. "nuances": è¯¦ç»†çš„è¯­æ³•ç¬”è®°ã€æƒ¯ç”¨è¯­è§£é‡Šæˆ–æ–‡åŒ–èƒŒæ™¯è¯´æ˜ã€‚
    5. "structure": ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«é€è¯æ‹†è§£ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
       - "word": åŸæ–‡å•è¯/è¯ç»„
       - "reading": å‘éŸ³æ³¨éŸ³ (è‹±è¯­è¯·æä¾›IPAéŸ³æ ‡ï¼Œæ—¥è¯­æä¾›ç½—é©¬éŸ³ï¼Œä¸­æ–‡æä¾›æ‹¼éŸ³)
       - "pos_meaning": è¯æ€§åŠä¸­æ–‡å«ä¹‰
       - "grammar": ç®€çŸ­è¯­æ³•è¯´æ˜ (æ—¶æ€ã€å˜ä½ç­‰)
       - "standard": åŸå‹/æ ‡å‡†å½¢å¼ (å¦‚åŠ¨è¯åŸå½¢)

    è¯·ç¡®ä¿è¾“å‡ºæ˜¯åˆæ³•çš„ JSON æ ¼å¼ã€‚
    """
    
    try:
        response = model.generate_content(prompt)
        clean_text = response.text.replace('```json', '').replace('```', '').strip()
        result = json.loads(clean_text)
        
        if "structure" not in result or "translation" not in result:
             return {"error": "AIè¿”å›æ ¼å¼ä¸å®Œæ•´", "structure": []}
            
        return result
        
    except Exception as e:
        return {"error": f"AIåˆ†æå¤±è´¥: {e}", "structure": []}

# --- 3. é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="å…¨èƒ½è¯­è¨€ä¼´ä¾£",
    page_icon="ğŸŒ",
    layout="centered",
    initial_sidebar_state="expanded" 
)

# æ—¶å°šçš„ UI æ ·å¼
st.markdown("""
<style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
    td { white-space: normal !important; word-wrap: break-word !important; }
    
    .stApp { background-color: #fafafa; }
    .main .block-container { 
        background-color: #ffffff; 
        padding: 2rem; 
        border-radius: 15px; 
        box-shadow: 0 4px 6px rgba(0,0,0,0.05); 
    }
    
    .lang-tag {
        background-color: #e3f2fd;
        color: #1976d2;
        padding: 4px 10px;
        border-radius: 16px;
        font-size: 13px;
        font-weight: 600;
        margin-right: 10px;
        display: inline-block;
    }
    
    .trans-text { font-size: 18px; color: #333; line-height: 1.6; }
    
    .nuance-box {
        background-color: #fff8e1;
        border-left: 4px solid #ffc107;
        padding: 15px;
        border-radius: 4px;
        color: #5d4037;
    }
    /* ğŸŒŸ æ–°å¢ï¼šä¿®æ­£æ¡†æ ·å¼ */
    .correction-box {
        background-color: #e6ffe6; /* æµ…ç»¿è‰²èƒŒæ™¯ */
        padding: 15px;
        border-radius: 8px;
        margin-top: 15px;
        border: 1px solid #4CAF50;
    }
    .correction-box strong { color: #2E7D32; }
</style>
""", unsafe_allow_html=True)


if 'user_id' not in st.session_state:
    st.session_state['user_id'] = 'ç”¨æˆ·A'

# --- 5. ä¾§è¾¹æ ï¼šä¸ªäººä¸­å¿ƒä¸ç»Ÿè®¡ ---
with st.sidebar:
    st.header("ğŸ‘¤ ä¸ªäººä¸­å¿ƒ")
    st.session_state['user_id'] = st.text_input("æ˜µç§°", value=st.session_state['user_id'])
    
    st.markdown("---")
    st.subheader("ğŸ“Š å­¦ä¹ ä»ªè¡¨ç›˜")
    
    hist_df_stats = load_history()
    if not hist_df_stats.empty:
        total_queries = len(hist_df_stats)
        langs_learned = hist_df_stats['language'].nunique()
        top_lang = hist_df_stats['language'].mode()[0] if not hist_df_stats.empty else "æ— "
        
        c1, c2 = st.columns(2)
        c1.metric("æ€»æŸ¥è¯¢", total_queries)
        c2.metric("æ¶‰çŒè¯­è¨€", langs_learned)
        st.metric("æœ€çˆ±è¯­è¨€", top_lang)
    else:
        st.info("æš‚æ— å­¦ä¹ æ•°æ®")
        
    st.markdown("---")
    st.markdown("ğŸ’¡ *Made with Streamlit & Gemini*")

# --- 6. ä¸»ç•Œé¢ UI ---
st.title("ğŸŒ å…¨èƒ½è¯­è¨€ä¼´ä¾£")
st.caption("AI é©±åŠ¨çš„å¤šè¯­ç§ç¿»è¯‘ã€è¯­æ³•è§£æä¸å‘éŸ³åŠ©æ‰‹")

# è¾“å…¥åŒº
with st.container():
    sentence = st.text_area("", height=100, placeholder="åœ¨æ­¤è¾“å…¥æ—¥è¯­ã€è‹±è¯­ã€éŸ©è¯­æˆ–ä»»ä½•ä½ æƒ³å­¦ä¹ çš„è¯­è¨€å¥å­...")
    
    col_btn, col_empty = st.columns([1, 3])
    with col_btn:
        analyze_btn = st.button("âœ¨ æ·±åº¦è§£æ", type="primary", use_container_width=True)

    if analyze_btn:
        if not sentence:
            st.toast("âš ï¸ è¯·å…ˆè¾“å…¥å¥å­ï¼", icon="âœï¸")
        else:
            with st.spinner('ğŸ¤– AI æ­£åœ¨è¯†åˆ«è¯­è¨€ã€æ£€æŸ¥é”™è¯¯å¹¶æ‹†è§£è¯­æ³•...'):
                ai_result = analyze_with_ai(sentence)
                
                if "error" in ai_result:
                    st.error(ai_result["error"])
                else:
                    save_record(sentence, ai_result)
                    load_history.clear() 
                    
                    st.toast("âœ… è§£æå®Œæˆï¼å·²ä¿å­˜åˆ°äº‘ç«¯ã€‚", icon="ğŸ‰")
                    
                    # --- ç»“æœå±•ç¤ºåŒº (ä½¿ç”¨ Tabs ä¼˜åŒ–å¸ƒå±€) ---
                    st.markdown("###")
                    
                    lang_name = ai_result.get('language', 'è‹±è¯­')
                    correction = ai_result.get('correction', sentence) # ğŸŒŸ å®ç”¨åŠŸèƒ½ä¸€
                    
                    audio_fp = text_to_speech(sentence, lang_name)
                    
                    # é¡¶éƒ¨åŸºæœ¬ä¿¡æ¯å¡ç‰‡
                    with st.container():
                        c_lang, c_audio = st.columns([0.2, 0.8])
                        with c_lang:
                            st.markdown(f"<span class='lang-tag'>{lang_name}</span>", unsafe_allow_html=True)
                        with c_audio:
                            if audio_fp:
                                st.audio(audio_fp.getvalue(), format='audio/mp3')
                            else:
                                st.warning("ğŸ”Š æ— æ³•ç”Ÿæˆæˆ–æ’­æ”¾éŸ³é¢‘ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ›´æ¢ç§»åŠ¨æµè§ˆå™¨ã€‚")
                    
                    # ä½¿ç”¨ Tabs åˆ†é¡µå±•ç¤º
                    tab1, tab2, tab3 = st.tabs(["ğŸ“ ç¿»è¯‘ä¸ç¬”è®°", "ğŸ§© ç»“æ„æ‹†è§£", "ğŸ” åŸå§‹æ•°æ®"])
                    
                    with tab1:
                        # ğŸŒŸ å®ç”¨åŠŸèƒ½ä¸€ï¼šçº é”™ç»“æœå±•ç¤º
                        if correction != sentence:
                            st.markdown(f"""
                            <div class="correction-box">
                                <strong>âš ï¸ ä¿®æ­£/æ¶¦è‰²åçš„ç‰ˆæœ¬:</strong><br>{correction}
                                <br><br>
                                <strong>åŸæ–‡:</strong><br><del style="color: grey;">{sentence}</del>
                            </div>
                            """, unsafe_allow_html=True)
                        else:
                             st.markdown(f"""
                            <div class="correction-box">
                                <strong>âœ… æ­å–œ!</strong><br>æ‚¨çš„å¥å­è¡¨è¾¾è‡ªç„¶ä¸”å‡†ç¡®ã€‚
                            </div>
                            """, unsafe_allow_html=True)

                        st.markdown("#### ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç¿»è¯‘")
                        st.markdown(f"<div class='trans-text'>{ai_result.get('translation', '')}</div>", unsafe_allow_html=True)
                        
                        st.markdown("#### ğŸ’¡ è¯­æ³•ä¸æ–‡åŒ–ç¬”è®°")
                        st.markdown(f"""
                        <div class="nuance-box">
                            {ai_result.get('nuances', 'æ— ç‰¹æ®Šè¯´æ˜').replace(chr(10), '<br>')}
                        </div>
                        """, unsafe_allow_html=True)
                        
                    with tab2:
                        st.markdown("#### é€è¯æ‹†è§£")
                        df = pd.DataFrame(ai_result.get('structure', []))
                        if not df.empty:
                            df_display = df.rename(columns=COLUMN_MAPPING)
                            st.table(df_display)
                        else:
                            st.info("æ— æ³•ç”Ÿæˆç»“æ„è¡¨æ ¼")
                            
                    with tab3:
                        st.json(ai_result)

st.divider()

# --- 7. å­¦ä¹ è¶³è¿¹ (æ–°å¢å¤ä¹ æ¨¡å¼) ---
st.subheader("ğŸ“š å­¦ä¹ è¶³è¿¹")

# åˆå§‹åŒ– session_state
if 'select_all' not in st.session_state:
    st.session_state.select_all = False
if 'delete_selections' not in st.session_state:
    st.session_state.delete_selections = {}
if 'search_query' not in st.session_state:
    st.session_state.search_query = ''
if 'filter_language' not in st.session_state:
    st.session_state.filter_language = None
# ğŸŒŸ å®ç”¨åŠŸèƒ½äºŒï¼šå¤ä¹ æ¨¡å¼çŠ¶æ€
if 'review_mode' not in st.session_state:
    st.session_state.review_mode = False

# åŠ è½½æ•°æ®
history_df = load_history()

if not history_df.empty and 'timestamp' in history_df.columns:
    
    # é¡¶éƒ¨å·¥å…·æ ï¼šç­›é€‰ + å¤ä¹ æ¨¡å¼ + å¯¼å‡º
    col_filter, col_review, col_export = st.columns([0.6, 0.2, 0.2])
    
    with col_filter:
        available_languages = history_df['language'].unique().tolist()
        if len(available_languages) > 0:
            st.markdown("##### è¯­è¨€ç­›é€‰")
            cols = st.columns(len(available_languages) + 1)
            def set_lang_filter(lang):
                if st.session_state.filter_language == lang:
                    st.session_state.filter_language = None
                else:
                    st.session_state.filter_language = lang
                st.session_state.select_all = False 
                st.session_state.delete_selections = {}

            for i, lang in enumerate(available_languages):
                btn_type = "primary" if st.session_state.filter_language == lang else "secondary"
                if cols[i].button(lang, key=f"filter_btn_{lang}", type=btn_type):
                    set_lang_filter(lang)
                    st.rerun()

    with col_review:
        # ğŸŒŸ å®ç”¨åŠŸèƒ½äºŒï¼šå¤ä¹ æ¨¡å¼å¼€å…³
        st.markdown("##### å¤ä¹ æ¨¡å¼")
        st.checkbox("å¼€å¯é—ªå¡", key='review_mode', value=st.session_state.review_mode)
    
    with col_export:
        st.markdown("##### å¯¼å‡ºæ•°æ®")
        csv = history_df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ å¯¼å‡º CSV",
            data=csv,
            file_name=f'learning_history_{datetime.now().strftime("%Y%m%d")}.csv',
            mime='text/csv',
            use_container_width=True
        )

    # æ‰§è¡Œè¿‡æ»¤
    filtered_df = history_df.copy()
    if st.session_state.filter_language:
        filtered_df = filtered_df[filtered_df['language'] == st.session_state.filter_language]

    search_query = st.text_input("ğŸ” æœç´¢å†å²:", placeholder="æœç´¢åŸæ–‡ã€ç¿»è¯‘æˆ–ç¬”è®°...", key='search_query')
    if search_query:
        filtered_df = filtered_df[
            filtered_df['sentence'].str.contains(search_query, case=False, na=False) | 
            (filtered_df['data'].astype(str).str.contains(search_query, case=False, na=False))
        ]

    # æ‰¹é‡åˆ é™¤é€»è¾‘
    if not filtered_df.empty:
        c_sel, c_del, c_space = st.columns([0.15, 0.35, 0.5])
        c_sel.checkbox("å…¨é€‰", key="select_all", on_change=update_selections)

        timestamps_to_delete = [
            ts for ts, is_checked in st.session_state.delete_selections.items() 
            if is_checked and ts in filtered_df['timestamp'].values
        ]
        
        c_del.button(
            "ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", 
            type="primary", 
            key="bulk_delete_main_btn",
            on_click=bulk_delete_callback,
            args=(timestamps_to_delete,)
        )

    # åˆ—è¡¨æ˜¾ç¤º
    if filtered_df.empty:
        st.info("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•")
    else:
        for index, item in filtered_df.iterrows():
            timestamp = item['timestamp']
            lang_label = item.get('language', 'æœªçŸ¥')
            
            # å¤ä¹ æ¨¡å¼ä¸‹ï¼Œåªæ˜¾ç¤ºå¥å­ï¼Œä¸è¿›è¡Œæˆªæ–­
            if st.session_state.review_mode:
                 display_sentence = item['sentence']
            else:
                 display_sentence = item['sentence'][:30] + '...' if len(item['sentence']) > 30 else item['sentence']
            
            with st.container():
                c_check, c_content = st.columns([0.05, 0.95])
                
                with c_check:
                    checkbox_key = f"sel_{timestamp}"
                    if checkbox_key not in st.session_state:
                        st.session_state[checkbox_key] = st.session_state.delete_selections.get(timestamp, False)
                    st.checkbox("", key=checkbox_key, on_change=update_individual_selection, args=(timestamp,), label_visibility="hidden")
                
                with c_content:
                    # ğŸŒŸ å¤ä¹ æ¨¡å¼ä¸‹çš„ Expander æ ‡é¢˜
                    expander_label = f"[{lang_label}] {display_sentence}"
                    
                    # ğŸŒŸ å®ç”¨åŠŸèƒ½äºŒï¼šå¤ä¹ æ¨¡å¼å†…å®¹æ§åˆ¶
                    if st.session_state.review_mode:
                        # å¤ä¹ æ¨¡å¼ä¸‹ï¼Œé»˜è®¤æŠ˜å ï¼Œåªæ˜¾ç¤ºåŸæ–‡
                        with st.expander(expander_label):
                            # ä½¿ç”¨ session state åŠ¨æ€æ§åˆ¶ç­”æ¡ˆæ˜¾ç¤º
                            reveal_key = f'reveal_{timestamp}'
                            if reveal_key not in st.session_state:
                                st.session_state[reveal_key] = False
                                
                            if st.session_state[reveal_key]:
                                st.button("éšè—ç­”æ¡ˆ", key=f'hide_btn_{timestamp}', on_click=lambda: st.session_state.update({reveal_key: False}))
                                show_answer = True
                            else:
                                st.button("æ˜¾ç¤ºç­”æ¡ˆ", key=f'show_btn_{timestamp}', type="primary", on_click=lambda: st.session_state.update({reveal_key: True}))
                                show_answer = False
                            
                            st.markdown("---")
                            
                            if show_answer:
                                st.caption(f"ğŸ‘¤ {item['user']} | ğŸ•’ {timestamp}")
                                data = item.get('data', {})
                                if data and "structure" in data:
                                    st.markdown(f"**ç¿»è¯‘ï¼š** {data.get('translation', '')}")
                                    h_tab1, h_tab2 = st.tabs(["ç»“æ„è¡¨", "ç¬”è®°"])
                                    with h_tab1:
                                        h_df = pd.DataFrame(data['structure'])
                                        st.table(h_df.rename(columns=COLUMN_MAPPING))
                                    with h_tab2:
                                        st.info(data.get('nuances', 'æ— ç¬”è®°'))
                                else:
                                    st.warning("æ•°æ®æ— æ³•è§£æ")
                    else:
                        # æ­£å¸¸æ¨¡å¼ä¸‹ï¼Œå±•å¼€å³æ˜¾ç¤ºæ‰€æœ‰å†…å®¹
                        with st.expander(expander_label):
                            st.caption(f"ğŸ•’ {timestamp} | ğŸ‘¤ {item['user']}")
                            data = item.get('data', {})
                            if data and "structure" in data:
                                if st.button("ğŸ”Š æœ—è¯»", key=f"tts_{timestamp}"):
                                    audio_bytes = text_to_speech(item['sentence'], lang_label)
                                    if audio_bytes:
                                        st.audio(audio_bytes.getvalue(), format='audio/mp3')
                                    else:
                                        st.toast("ğŸ”Š ç§»åŠ¨ç«¯æ’­æ”¾å¤±è´¥ã€‚", icon="âš ï¸")

                                st.markdown(f"**ç¿»è¯‘ï¼š** {data.get('translation', '')}")
                                
                                # ğŸŒŸ ä¿®æ­£å¯¹æ¯”ï¼ˆå†å²è®°å½•ï¼‰
                                correction_hist = data.get('correction', item['sentence'])
                                if correction_hist != item['sentence']:
                                    st.info(f"ğŸ’¡ **ä¿®æ­£ç‰ˆæœ¬:** {correction_hist}")

                                h_tab1, h_tab2 = st.tabs(["ç»“æ„è¡¨", "ç¬”è®°"])
                                with h_tab1:
                                    h_df = pd.DataFrame(data['structure'])
                                    st.table(h_df.rename(columns=COLUMN_MAPPING))
                                with h_tab2:
                                    st.info(data.get('nuances', 'æ— ç¬”è®°'))
                            else:
                                st.warning("æ•°æ®æ— æ³•è§£æ")

else:
    st.info("ğŸŒŸ æ¬¢è¿ä½¿ç”¨ï¼è¾“å…¥ç¬¬ä¸€ä¸ªå¥å­å¼€å§‹ä½ çš„è¯­è¨€ä¹‹æ—…å§ï¼")
